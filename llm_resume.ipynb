{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "if not os.environ.get(\"LANGCHAIN_API_KEY\"):\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"resume.txt\", encoding = 'UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf loader\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# file_path = \".pdf\"\n",
    "# loader = PyPDFLoader(file_path)\n",
    "\n",
    "# docs = loader.load()\n",
    "\n",
    "# print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Navneet Yadav \\nData Science | Deep Learning | Computer Vision | Python \\nynavneet816@gmail.com | +916265632142 | www.linkedin.com/in/navneet-yadav3 \\nI am a developer especially interested in building product or model pipeline using data in a best available way. \\nExited to solve real world problems and implement them with the help of all available tech. \\nWORK EXPERIENCE \\nData Science Engineer \\n Sep 2021 – June 2022 \\nXalt Analytics Indore, India \\n● Design and implement complete deep learning pipeline for meter reading recognition. \\n● Work on data cleaning to deployment i.e., whole machine learning pipeline. \\n● Work on live data product on data science, backend, and deployment part. \\n● Work on data analysis and visualization. \\n● Handle version control for organization. \\nEDUCATION \\nChameli Devi Group of institutions \\n June 2017 – June 2021 \\nB. Tech, Computer Science - 77.4% Indore, India\\nTECHNOLOGIES AND LANGUAGES\\n● Languages: Python, C, C++, SQL \\n● Technologies: Deep learning, Computer Vision, Machine Learning, Data Visualization, Data Analysis \\n● Other: TensorFlow, PyTorch, OpenCV, darknet, Git, Docker, Django, Celery, Flask, Power BI, AWS, Linux, \\nversion control (Git, GitLab, GitHub), PostgreSQL, tflite, LLM, Python, and Linux scripts for automation. \\nPROJECTS \\n● MPEB Meter Reading- make a deep learning model pipeline for recognizing meter reading, also make \\ntflite models for mobile devices with accuracy of 95%. Tried different architecture and models like Faster \\nRCNN, Xception, darknet-yolov4, pytorch-yolov5.\\n● Crystal Crop Analysis- Data analysis to solve the supply chain issue. Done EDA, data visualization and time \\nseries analysis to find the reason behind supply chain issue\\n● Xtract (Web App) - A platform where user can upload and tag the images, train model, and test the \\ntrained model. Added the meter reading pipeline in the existing platform, also work on backend which \\nincludes- (Django-Api, Celery and PostgreSQL), and containerize the whole application with docker for \\ndeployment with gunicorn and nginx. \\n● AVL Person tracking- make a model to track persons in sensitive places and offices. Use yolov4 for \\ndetection and Deep SORT for tracking. \\n● AVL Face Recognition – make face recognition system for attendance. Use MTCNN for face detection and \\nvggface2-resnet-50 for feature extraction. \\n● AVL Meter Reading- use to get reading from flow meter and combustion meter. Use yolov4, xception \\nmodel for finding area of interest and recognizing digits and character.', metadata={'source': 'resume.txt'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Who is navneet?\")\n",
    "\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.if question is not related to context then dont answer them.\"\n",
    "    \"your creater is navneet\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are two multiple-choice questions to assess Navneet's Python skills:\n",
      "\n",
      "1. Which of the following libraries is commonly used for deep learning in Python?\n",
      "   a) NumPy\n",
      "   b) Pandas\n",
      "   c) TensorFlow\n",
      "   d) Flask\n",
      "\n",
      "2. What is the primary purpose of the 'SQL' language in programming?\n",
      "   a) Web development\n",
      "   b) Data analysis\n",
      "   c) Database management\n",
      "   d) Machine learning\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"make two mcq question to check navneet's python skills level\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is an open-source framework developed by Google for building and deploying machine learning models. It provides a comprehensive ecosystem of tools, libraries, and community resources that enables researchers and developers to create state-of-the-art models for various tasks, including deep learning, computer vision, natural language processing, and more. TensorFlow supports both CPU and GPU computing, making it efficient for large-scale machine learning applications.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is use of tensorflow\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
